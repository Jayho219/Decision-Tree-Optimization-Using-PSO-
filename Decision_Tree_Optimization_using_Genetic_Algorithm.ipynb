{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZYJCWuZx-6u-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=None, min_samples_leaf=None, max_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "\n",
        "    def gini(self, labels):\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        proportions = counts / np.sum(counts)\n",
        "        return 1 - np.sum(proportions ** 2)\n",
        "\n",
        "    def split(self, X, y, feature_idx, threshold):\n",
        "        left_idxs = np.where(X[:, feature_idx] <= threshold)[0]\n",
        "        right_idxs = np.where(X[:, feature_idx] > threshold)[0]\n",
        "        left_X, left_y = X[left_idxs], y[left_idxs]\n",
        "        right_X, right_y = X[right_idxs], y[right_idxs]\n",
        "        return left_X, left_y, right_X, right_y\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        best_feature_idx, best_threshold, best_gini = None, None, np.inf\n",
        "        features = np.arange(X.shape[1])\n",
        "        if self.max_features is not None and self.max_features <= X.shape[1]:\n",
        "            features = np.random.choice(features, size=int(self.max_features), replace=False)\n",
        "        for feature_idx in features:\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                left_X, left_y, right_X, right_y = self.split(X, y, feature_idx, threshold)\n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "                if self.min_samples_leaf is not None:\n",
        "                    if len(left_y) < self.min_samples_leaf or len(right_y) < self.min_samples_leaf:\n",
        "                        continue\n",
        "                left_gini = self.gini(left_y)\n",
        "                right_gini = self.gini(right_y)\n",
        "                weighted_gini = (len(left_y) / len(y)) * left_gini + (len(right_y) / len(y)) * right_gini\n",
        "                if weighted_gini < best_gini:\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "                    best_gini = weighted_gini\n",
        "        return best_feature_idx, best_threshold\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
        "            return np.bincount(y).argmax()\n",
        "        if self.min_samples_split is not None and len(y) < self.min_samples_split:\n",
        "            return np.bincount(y).argmax()\n",
        "        if self.min_samples_leaf is not None and len(y) < 2 * self.min_samples_leaf:\n",
        "            return np.bincount(y).argmax()\n",
        "        feature_idx, threshold = self.best_split(X, y)\n",
        "        if feature_idx is None:\n",
        "            return np.bincount(y).argmax()\n",
        "        left_X, left_y, right_X, right_y = self.split(X, y, feature_idx, threshold)\n",
        "        left_subtree = self.fit(left_X, left_y, depth + 1)\n",
        "        right_subtree = self.fit(right_X, right_y, depth + 1)\n",
        "        return {'feature_idx': feature_idx, 'threshold': threshold, 'left': left_subtree, 'right': right_subtree}\n",
        "\n",
        "    def predict(self, X, tree):\n",
        "        if isinstance(tree, np.int64):\n",
        "            return tree\n",
        "        feature_idx, threshold = tree['feature_idx'], tree['threshold']\n",
        "        if X[feature_idx] <= threshold:\n",
        "            if isinstance(tree['left'], np.int64):\n",
        "                return tree['left']\n",
        "            else:\n",
        "                return self.predict(X, tree['left'])\n",
        "        else:\n",
        "            if isinstance(tree['right'], np.int64):\n",
        "                return tree['right']\n",
        "            else:\n",
        "                return self.predict(X, tree['right'])\n",
        "\n",
        "\n",
        "# Train Test Split\n",
        "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
        "    if random_state:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    n_samples = X.shape[0]\n",
        "    n_test = int(n_samples * test_size)\n",
        "\n",
        "    # Shuffle the data and target arrays\n",
        "    shuffled_idxs = np.random.permutation(n_samples)\n",
        "    X_shuffled = X[shuffled_idxs]\n",
        "    y_shuffled = y[shuffled_idxs]\n",
        "\n",
        "    # Split the data and target arrays into training and test sets\n",
        "    X_train, X_test = X_shuffled[:-n_test], X_shuffled[-n_test:]\n",
        "    y_train, y_test = y_shuffled[:-n_test], y_shuffled[-n_test:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Label Encoding\n",
        "def encode_data(data):\n",
        "    label_encoder = LabelEncoder()\n",
        "    for column in data.columns:\n",
        "         # check if the column contains boolean data\n",
        "        if data[column].dtype == bool:\n",
        "            data[column] = data[column].astype(int)\n",
        "        # check if the column contains categorical data\n",
        "        elif data[column].dtype == object or pd.api.types.is_string_dtype(data[column]):\n",
        "            data[column] = label_encoder.fit_transform(data[column])\n",
        "        # check if the column contains numeric data\n",
        "        elif pd.api.types.is_numeric_dtype(data[column]):\n",
        "            pass  # no encoding needed for numeric data\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported data type in column {column}: {data[column].dtype}\")\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file = pd.read_csv(\"weather.csv\")\n",
        "# print(file)\n",
        "\n",
        "# data = encode_data(file)\n",
        "# print(data)\n",
        "\n",
        "# X = data.drop(['play'], axis = 1)\n",
        "# y = data['play']\n",
        "# np_X = X.to_numpy()\n",
        "# np_y = y.to_numpy()\n",
        "# # Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np_X, np_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Fit the decision tree on the training data\n",
        "# dt = DecisionTree()\n",
        "# tree = dt.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions on the testing data\n",
        "# y_pred = [dt.predict(X, tree) for X in X_test]\n",
        "\n",
        "# # Evaluate the accuracy of the model\n",
        "# accuracy = sum(y_pred == y_test) / len(y_test)\n",
        "# print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# print(tree)"
      ],
      "metadata": {
        "id": "3qgT4yuDBD1f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quptlQqaFvdH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Genetic Algorithm Optimization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "To0gT0N7H6Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def fitness(individual):\n",
        "    clf = DecisionTree(\n",
        "        max_depth=individual[0],\n",
        "        min_samples_split=individual[1],\n",
        "        min_samples_leaf=individual[2],\n",
        "        max_features=individual[3]\n",
        "    )\n",
        "    tree = clf.fit(X_train, y_train)\n",
        "    y_pred = [clf.predict(X, tree) for X in X_test]\n",
        "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
        "    return accuracy\n",
        "\n",
        "def create_individual():\n",
        "    individual = []\n",
        "    individual.append(random.randint(1, 20))\n",
        "    individual.append(random.uniform(0.01, 0.1))\n",
        "    individual.append(random.uniform(0.01, 0.1))\n",
        "    individual.append(random.randint(5, 10)*0.1)\n",
        "    return individual\n",
        "\n",
        "def generate_population(population_size):\n",
        "  population = [create_individual() for i in range(population_size)]\n",
        "  return population\n",
        "\n",
        "def rank(fitness_values):\n",
        "    n = len(fitness_values)\n",
        "    ranks = [0] * n\n",
        "    sorted_indices = sorted(range(n), key=lambda x: fitness_values[x], reverse=True)\n",
        "    for i, idx in enumerate(sorted_indices):\n",
        "        ranks[idx] = i + 1\n",
        "    return ranks\n",
        "\n",
        "def selection_probabilities_rank(ranks):\n",
        "    population_size = len(ranks)\n",
        "\n",
        "    # Calculate the sum of the ranks\n",
        "    rank_sum = sum(range(1, population_size + 1))\n",
        "\n",
        "    # Calculate the selection probability for each individual based on its rank\n",
        "    selection_probs = [(population_size - rank + 1) / rank_sum for rank in ranks]\n",
        "\n",
        "    return selection_probs\n",
        "\n",
        "\n",
        "def select_parents_rank(population, selection_probabilities, num_parents):\n",
        "    parents = []\n",
        "    for i in range(num_parents):\n",
        "        selected_index = np.random.choice(len(population), p=selection_probabilities)\n",
        "        parents.append(population[selected_index])\n",
        "    return parents\n",
        "\n",
        "def crossover(parent1, parent2, crossover_rate):\n",
        "    child1 = parent1[:]\n",
        "    child2 = parent2[:]\n",
        "\n",
        "    for i in range(1, len(parent1)):\n",
        "        if random.random() < crossover_rate:\n",
        "            child1[i], child2[i] = child2[i], child1[i]\n",
        "\n",
        "    return child1, child2\n",
        "\n",
        "def mutate(individual, mutation_rate):\n",
        "    for i in range(1, len(individual)):\n",
        "        if random.random() < mutation_rate:\n",
        "            individual[i] = 1 - individual[i]*random.random()\n",
        "    return individual\n",
        "\n",
        "\n",
        "def genetic_algorithm(population_size, generations, crossover_rate, mutation_rate):\n",
        "    # Generate initial population\n",
        "    population = generate_population(population_size)\n",
        "    best_individual_ = create_individual()\n",
        "    best_fitness = fitness(best_individual_)\n",
        "\n",
        "    for i in range(generations):\n",
        "        # Calculate fitness for each individual\n",
        "        fitness_values = [fitness(individual) for individual in population]\n",
        "\n",
        "        best_individual = max(population, key=fitness)\n",
        "        curr_best_fitness = fitness(best_individual)\n",
        "        if curr_best_fitness > best_fitness:\n",
        "            best_individual_ = best_individual\n",
        "            best_fitness = curr_best_fitness\n",
        "\n",
        "        print(\"Iteration \", i)\n",
        "        print(\"curr_best: \", curr_best_fitness, \"  best_so_far: \", best_fitness)\n",
        "        print(best_individual)\n",
        "        print(best_individual_)\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "        # Implement elitism\n",
        "        elite_size = int(0.1 * population_size) # top 10% of population\n",
        "        sorted_population = [x for _, x in sorted(zip(fitness_values, population), reverse=True)]\n",
        "        elite = sorted_population[:elite_size]\n",
        "\n",
        "        # Select parents for crossover\n",
        "        # Implement rank-based selection\n",
        "        ranks = rank(fitness_values)\n",
        "        selection_probabilities = selection_probabilities_rank(ranks)\n",
        "        parent_indices = np.random.choice(np.arange(population_size), size=2, replace=False, p=selection_probabilities)\n",
        "        parents = select_parents_rank(population, selection_probabilities, 2*(population_size-elite_size))\n",
        "\n",
        "        # Crossover parents to create offspring\n",
        "        offspring = []\n",
        "        for j in range(0, len(parents), 2):\n",
        "            offspring.extend(crossover(parents[j], parents[j+1], crossover_rate))\n",
        "\n",
        "        # Mutate offspring\n",
        "        mutated_offspring = [mutate(individual, mutation_rate) for individual in offspring]\n",
        "\n",
        "        # Add elite to population\n",
        "        new_population = elite + mutated_offspring\n",
        "\n",
        "        # Keep population size constant\n",
        "        new_population = new_population[:population_size]\n",
        "\n",
        "        # Update population for next generation\n",
        "        population = new_population\n",
        "\n",
        "    # Return the best individual found during the entire evolution process\n",
        "\n",
        "    best_individual = max(population, key=fitness)\n",
        "    curr_best_fitness = fitness(best_individual)\n",
        "    if curr_best_fitness > best_fitness:\n",
        "        best_individual_ = best_individual\n",
        "        best_fitness = curr_best_fitness\n",
        "    print(\"curr_best: \", curr_best_fitness, \"  best_so_far: \", best_fitness)\n",
        "    print(best_individual)\n",
        "    print(best_individual_)\n",
        "    print()\n",
        "    return best_individual_, best_fitness"
      ],
      "metadata": {
        "id": "iLohzVcZH-p8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################# IRIS DATASET ################\n",
        "# from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# iris = load_iris()\n",
        "# x = iris.data\n",
        "# y = iris.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# dt = DecisionTree()\n",
        "# tree = dt.fit(X_train, y_train)\n",
        "# y_pred = [dt.predict(X, tree) for X in X_test]\n",
        "# accuracy = sum(y_pred == y_test) / len(y_test)\n",
        "# print(f\"Accuracy: {accuracy}\")\n",
        "# print(y_test)\n",
        "# print(y_pred)\n",
        "\n",
        "################ WINE DATASET ##################\n",
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine()\n",
        "x = wine.data\n",
        "y = wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = DecisionTree()\n",
        "tree = dt.fit(X_train, y_train)\n",
        "y_pred = [dt.predict(X, tree) for X in X_test]\n",
        "accuracy = sum(y_pred == y_test) / len(y_test)\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "print(f\"Accuracy with Scikit Learn Decision Tree Classifier: {accuracy}\")\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "# Find the best individual and print the decision tree\n",
        "print('=' * 100)\n",
        "print(\"Optimization Starts using Genetic Algorithm\")\n",
        "print('=' * 100)\n",
        "best_individual, best_fitness = genetic_algorithm(100, 15, 0.1, 0.1)\n",
        "print(best_individual)\n",
        "print(\"Best Accuracy after Optimization using Genetic Algorith: \", best_fitness)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5_xiX1xEWcp",
        "outputId": "3342a888-4b67-4caf-b4ff-620c77aec99a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "[0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0]\n",
            "Accuracy with Scikit Learn Decision Tree Classifier: 0.9166666666666666\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "Optimization Starts using Genetic Algorithm\n",
            "====================================================================================================\n",
            "Iteration  0\n",
            "curr_best:  0.75   best_so_far:  0.75\n",
            "[19, 0.06183600883072026, 0.07603987139141197, 1.0]\n",
            "[19, 0.06183600883072026, 0.07603987139141197, 1.0]\n",
            "\n",
            "\n",
            "Iteration  1\n",
            "curr_best:  0.7777777777777778   best_so_far:  0.7777777777777778\n",
            "[8, 0.05883271128011678, 0.01819514608685875, 1.0]\n",
            "[8, 0.05883271128011678, 0.01819514608685875, 1.0]\n",
            "\n",
            "\n",
            "Iteration  2\n",
            "curr_best:  0.8888888888888888   best_so_far:  0.8888888888888888\n",
            "[9, 0.9409255229019605, 0.07092808706355679, 1.0]\n",
            "[9, 0.9409255229019605, 0.07092808706355679, 1.0]\n",
            "\n",
            "\n",
            "Iteration  3\n",
            "curr_best:  0.8888888888888888   best_so_far:  0.8888888888888888\n",
            "[8, 0.08428484487106422, 0.01819514608685875, 1.0]\n",
            "[9, 0.9409255229019605, 0.07092808706355679, 1.0]\n",
            "\n",
            "\n",
            "Iteration  4\n",
            "curr_best:  0.8333333333333334   best_so_far:  0.8888888888888888\n",
            "[20, 0.08428484487106422, 0.964437305348502, 1.0]\n",
            "[9, 0.9409255229019605, 0.07092808706355679, 1.0]\n",
            "\n",
            "\n",
            "Iteration  5\n",
            "curr_best:  0.8333333333333334   best_so_far:  0.8888888888888888\n",
            "[20, 0.03100445739574055, 0.06987764609076459, 1.0]\n",
            "[9, 0.9409255229019605, 0.07092808706355679, 1.0]\n",
            "\n",
            "\n",
            "Iteration  6\n",
            "curr_best:  0.8611111111111112   best_so_far:  0.8888888888888888\n",
            "[11, 0.9163111447169127, 0.9471562456687763, 1.0]\n",
            "[9, 0.9409255229019605, 0.07092808706355679, 1.0]\n",
            "\n",
            "\n",
            "Iteration  7\n",
            "curr_best:  0.9166666666666666   best_so_far:  0.9166666666666666\n",
            "[10, 0.03276035843166087, 0.956853754690812, 1.0]\n",
            "[10, 0.03276035843166087, 0.956853754690812, 1.0]\n",
            "\n",
            "\n",
            "Iteration  8\n",
            "curr_best:  0.8333333333333334   best_so_far:  0.9166666666666666\n",
            "[11, 0.09388263804307441, 0.07926335724209194, 1.0]\n",
            "[10, 0.03276035843166087, 0.956853754690812, 1.0]\n",
            "\n",
            "\n",
            "Iteration  9\n",
            "curr_best:  0.9166666666666666   best_so_far:  0.9166666666666666\n",
            "[13, 0.0939601003212402, 0.07926335724209194, 1.0]\n",
            "[10, 0.03276035843166087, 0.956853754690812, 1.0]\n",
            "\n",
            "\n",
            "Iteration  10\n",
            "curr_best:  0.7777777777777778   best_so_far:  0.9166666666666666\n",
            "[20, 0.920903612009622, 0.508231190016421, 1.0]\n",
            "[10, 0.03276035843166087, 0.956853754690812, 1.0]\n",
            "\n",
            "\n",
            "Iteration  11\n",
            "curr_best:  0.8888888888888888   best_so_far:  0.9166666666666666\n",
            "[15, 0.8029193438707096, 0.9504774295243006, 1.0]\n",
            "[10, 0.03276035843166087, 0.956853754690812, 1.0]\n",
            "\n",
            "\n",
            "Iteration  12\n",
            "curr_best:  0.9444444444444444   best_so_far:  0.9444444444444444\n",
            "[11, 0.9979075075463485, 0.964437305348502, 1.0]\n",
            "[11, 0.9979075075463485, 0.964437305348502, 1.0]\n",
            "\n",
            "\n",
            "Iteration  13\n",
            "curr_best:  0.8055555555555556   best_so_far:  0.9444444444444444\n",
            "[20, 0.1712347677058692, 0.7045325349302296, 1.0]\n",
            "[11, 0.9979075075463485, 0.964437305348502, 1.0]\n",
            "\n",
            "\n",
            "Iteration  14\n",
            "curr_best:  0.7777777777777778   best_so_far:  0.9444444444444444\n",
            "[20, 0.1712347677058692, 0.7045325349302296, 1.0]\n",
            "[11, 0.9979075075463485, 0.964437305348502, 1.0]\n",
            "\n",
            "\n",
            "curr_best:  0.7222222222222222   best_so_far:  0.9444444444444444\n",
            "[11, 0.9979075075463485, 0.5876828872535853, 1.0]\n",
            "[11, 0.9979075075463485, 0.964437305348502, 1.0]\n",
            "\n",
            "[11, 0.9979075075463485, 0.964437305348502, 1.0]\n",
            "Best Accuracy after Optimization using Genetic Algorith:  0.9444444444444444\n"
          ]
        }
      ]
    }
  ]
}